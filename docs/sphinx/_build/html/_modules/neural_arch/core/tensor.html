

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>neural_arch.core.tensor &mdash; Neural Architecture 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=51b770b3"></script>
      <script src="../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            Neural Architecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorial.html">Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/core.html">Core Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/functional.html">Functional Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/nn.html">Neural Network Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/optim.html">Optimization Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/config.html">Configuration Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/cli.html">Command Line Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../genindex.html">Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../py-modindex.html">Module Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../search.html">Search Page</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Neural Architecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">neural_arch.core.tensor</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for neural_arch.core.tensor</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Enterprise-grade tensor implementation with automatic differentiation.&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Protocol</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">weakref</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.dtype</span><span class="w"> </span><span class="kn">import</span> <span class="n">DType</span><span class="p">,</span> <span class="n">get_default_dtype</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.device</span><span class="w"> </span><span class="kn">import</span> <span class="n">Device</span><span class="p">,</span> <span class="n">get_default_device</span>

<span class="c1"># Type aliases</span>
<span class="n">TensorLike</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="s1">&#39;Tensor&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
<span class="n">Shape</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

<span class="c1"># Global gradient computation state</span>
<span class="n">_grad_enabled</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GradientFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Represents a function in the computational graph for gradient computation.&quot;&quot;&quot;</span>
    
    <span class="n">backward_fn</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="s1">&#39;Tensor&#39;</span><span class="p">]</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Unknown&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the backward function with gradient clipping and error handling.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Apply gradient clipping for numerical stability</span>
            <span class="n">grad_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
            
            <span class="c1"># Check for NaN/Inf gradients</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Non-finite gradients detected in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">grad_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">)</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">backward_fn</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
            
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error in gradient computation for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span>


<div class="viewcode-block" id="Tensor">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Enterprise-grade tensor with automatic differentiation.</span>
<span class="sd">    </span>
<span class="sd">    This tensor implementation provides:</span>
<span class="sd">    - Automatic differentiation with computational graph tracking</span>
<span class="sd">    - Memory-efficient gradient computation</span>
<span class="sd">    - Numerical stability with gradient clipping</span>
<span class="sd">    - Device placement management</span>
<span class="sd">    - Type safety with runtime checks</span>
<span class="sd">    - Comprehensive error handling</span>
<span class="sd">    - Performance monitoring hooks</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="Tensor.__init__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">,</span>
        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize tensor with enterprise-grade features.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            data: Input data (array-like)</span>
<span class="sd">            requires_grad: Whether to track gradients</span>
<span class="sd">            dtype: Data type (defaults to global default)</span>
<span class="sd">            device: Device placement (defaults to global default)</span>
<span class="sd">            name: Optional name for debugging</span>
<span class="sd">            </span>
<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If data type is invalid</span>
<span class="sd">            ValueError: If data contains invalid values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Input validation</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;requires_grad must be bool, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Convert data to numpy array with proper dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_and_convert_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        
        <span class="c1"># Core tensor properties</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">_grad_enabled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span> <span class="ow">or</span> <span class="n">get_default_dtype</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span> <span class="ow">or</span> <span class="n">get_default_device</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
        
        <span class="c1"># Gradient computation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GradientFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_version</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># For detecting in-place modifications</span>
        
        <span class="c1"># Enterprise features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_creation_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_creation_context</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_memory_usage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">nbytes</span>
        
        <span class="c1"># Weak references to dependent tensors for graph cleanup</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dependents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Created tensor </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2"> with shape </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> on </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="nf">_validate_and_convert_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DType</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validate and convert input data to numpy array.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            data: Input data</span>
<span class="sd">            dtype: Target data type</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            Validated numpy array</span>
<span class="sd">            </span>
<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If data type is invalid</span>
<span class="sd">            ValueError: If data contains invalid values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported data type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Apply dtype conversion</span>
        <span class="n">target_dtype</span> <span class="o">=</span> <span class="n">dtype</span> <span class="ow">or</span> <span class="n">get_default_dtype</span><span class="p">()</span>
        <span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">target_dtype</span><span class="o">.</span><span class="n">numpy_dtype</span><span class="p">)</span>
        
        <span class="c1"># Validate for NaN/Inf values</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">array</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">array</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tensor data contains NaN values&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">array</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tensor data contains infinite values&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">array</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_get_creation_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get context information for tensor creation (for debugging).&quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">inspect</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Get the frame that called Tensor.__init__</span>
            <span class="n">caller_frame</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">f_back</span><span class="o">.</span><span class="n">f_back</span> <span class="k">if</span> <span class="n">frame</span> <span class="ow">and</span> <span class="n">frame</span><span class="o">.</span><span class="n">f_back</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">caller_frame</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="s1">&#39;filename&#39;</span><span class="p">:</span> <span class="n">caller_frame</span><span class="o">.</span><span class="n">f_code</span><span class="o">.</span><span class="n">co_filename</span><span class="p">,</span>
                    <span class="s1">&#39;lineno&#39;</span><span class="p">:</span> <span class="n">caller_frame</span><span class="o">.</span><span class="n">f_lineno</span><span class="p">,</span>
                    <span class="s1">&#39;function&#39;</span><span class="p">:</span> <span class="n">caller_frame</span><span class="o">.</span><span class="n">f_code</span><span class="o">.</span><span class="n">co_name</span><span class="p">,</span>
                <span class="p">}</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">frame</span>
        <span class="k">return</span> <span class="p">{}</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the underlying data array.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span>
    
    <span class="nd">@data</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the underlying data array with validation.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected np.ndarray, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape mismatch: expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="o">.</span><span class="n">numpy_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_version</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Track in-place modifications</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Shape</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get tensor shape.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get number of dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">ndim</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get total number of elements.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">size</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DType</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get tensor data type.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Device</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get tensor device.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">requires_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if gradients are tracked.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_requires_grad</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get accumulated gradients.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span>
    
    <span class="nd">@grad</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set accumulated gradients.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected np.ndarray or None, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient shape </span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> doesn&#39;t match tensor shape </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">value</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">grad_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GradientFunction</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get gradient function for backpropagation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_fn</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get tensor name.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>
    
<div class="viewcode-block" id="Tensor.zero_grad">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.zero_grad">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset gradients to None with memory cleanup.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Zeroed gradients for tensor </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.backward">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.backward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute gradients with enterprise-grade error handling.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            gradient: Upstream gradient (defaults to ones)</span>
<span class="sd">            </span>
<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If tensor doesn&#39;t require gradients</span>
<span class="sd">            ValueError: If gradient shape is invalid</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_requires_grad</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attempted backward on tensor </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2"> that doesn&#39;t require grad&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_grad_enabled</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Gradient computation is disabled globally&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        
        <span class="c1"># Default gradient for scalar outputs</span>
        <span class="k">if</span> <span class="n">gradient</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Gradient must be specified for non-scalar tensors&quot;</span><span class="p">)</span>
        
        <span class="c1"># Validate gradient shape</span>
        <span class="k">if</span> <span class="n">gradient</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient shape </span><span class="si">{</span><span class="n">gradient</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> doesn&#39;t match tensor shape </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Handle NaN/Inf gradients and extreme values for numerical stability</span>
        <span class="n">processed_gradient</span> <span class="o">=</span> <span class="n">gradient</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        
        <span class="c1"># Handle non-finite gradients</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">processed_gradient</span><span class="p">)):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Non-finite gradients detected in tensor </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">processed_gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">processed_gradient</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">)</span>
        
        <span class="c1"># Apply gradient clipping only for extremely large gradients</span>
        <span class="n">max_abs_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">processed_gradient</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">max_abs_grad</span> <span class="o">&gt;=</span> <span class="mf">1e6</span><span class="p">:</span>  <span class="c1"># Clip gradients &gt;= 1 million for numerical stability</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Extremely large gradient detected (</span><span class="si">{</span><span class="n">max_abs_grad</span><span class="si">}</span><span class="s2">), applying clipping&quot;</span><span class="p">)</span>
            <span class="n">processed_gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">processed_gradient</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
        
        <span class="c1"># Accumulate gradients</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">processed_gradient</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">+=</span> <span class="n">processed_gradient</span>
        
        <span class="c1"># Propagate gradients through computational graph</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_grad_fn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
        
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computed gradients for tensor </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.detach">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.detach">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">detach</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new tensor detached from the computational graph.</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            New tensor with same data but no gradient tracking</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">_detached&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.clone">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.clone">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a deep copy of the tensor.</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            New tensor with copied data and gradient tracking</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cloned</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_requires_grad</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">_clone&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">cloned</span></div>

    
<div class="viewcode-block" id="Tensor.to">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.to">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move tensor to specified device/dtype.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            device: Target device</span>
<span class="sd">            dtype: Target data type</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            New tensor on specified device/dtype</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_device</span> <span class="o">=</span> <span class="n">device</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>
        <span class="n">new_dtype</span> <span class="o">=</span> <span class="n">dtype</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>
        
        <span class="c1"># Convert data if dtype changed</span>
        <span class="n">new_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span>
        <span class="k">if</span> <span class="n">new_dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">:</span>
            <span class="n">new_data</span> <span class="o">=</span> <span class="n">new_data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">new_dtype</span><span class="o">.</span><span class="n">numpy_dtype</span><span class="p">)</span>
        
        <span class="c1"># For now, device movement is a no-op (CPU only)</span>
        <span class="c1"># In a real implementation, this would handle GPU transfers</span>
        
        <span class="k">if</span> <span class="n">new_device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="ow">and</span> <span class="n">new_dtype</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span>
            <span class="n">new_data</span><span class="p">,</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_requires_grad</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">new_dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">new_device</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.numpy">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.numpy">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert to numpy array (detached from gradient computation).</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            NumPy array with tensor data</span>
<span class="sd">            </span>
<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If tensor requires gradients</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot convert tensor with requires_grad=True to numpy. Use detach() first.&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span></div>

    
<div class="viewcode-block" id="Tensor.item">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.item">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">item</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract scalar value from tensor.</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">            Scalar value</span>
<span class="sd">            </span>
<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If tensor is not scalar</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can only extract scalar from single-element tensor, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s2"> elements&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span></div>

    
<div class="viewcode-block" id="Tensor.memory_usage">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.memory_usage">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">memory_usage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get memory usage in bytes.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_memory_usage</span></div>

    
<div class="viewcode-block" id="Tensor.__add__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__add__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Addition operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">add</span>
        <span class="k">return</span> <span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__radd__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__radd__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reverse addition operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">add</span>
        <span class="k">return</span> <span class="n">add</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__mul__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__mul__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Multiplication operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">mul</span>
        <span class="k">return</span> <span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__rmul__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__rmul__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reverse multiplication operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">mul</span>
        <span class="k">return</span> <span class="n">mul</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__matmul__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__matmul__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Matrix multiplication operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">matmul</span>
        <span class="k">return</span> <span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__sub__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__sub__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Subtraction operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">sub</span>
        <span class="k">return</span> <span class="n">sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__rsub__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__rsub__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reverse subtraction operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">sub</span>
        <span class="k">return</span> <span class="n">sub</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__truediv__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__truediv__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Division operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">div</span>
        <span class="k">return</span> <span class="n">div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__rtruediv__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__rtruediv__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">TensorLike</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reverse division operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">div</span>
        <span class="k">return</span> <span class="n">div</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__neg__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__neg__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Negation operator.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">..functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">neg</span>
        <span class="k">return</span> <span class="n">neg</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="Tensor.__repr__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__repr__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String representation of tensor.&quot;&quot;&quot;</span>
        <span class="n">grad_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;, requires_grad=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_requires_grad</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_requires_grad</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="n">device_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;, device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="o">.</span><span class="n">is_cpu</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="n">name_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;, name=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">!r}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Tensor(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="si">}{</span><span class="n">grad_str</span><span class="si">}{</span><span class="n">device_str</span><span class="si">}{</span><span class="n">name_str</span><span class="si">}</span><span class="s2">)&quot;</span></div>

    
<div class="viewcode-block" id="Tensor.__str__">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.Tensor.__str__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Human-readable string representation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Tensor(shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="si">}</span><span class="s2">)&quot;</span></div>
</div>



<div class="viewcode-block" id="no_grad">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.no_grad">[docs]</a>
<span class="nd">@contextmanager</span>
<span class="k">def</span><span class="w"> </span><span class="nf">no_grad</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context manager to disable gradient computation.</span>
<span class="sd">    </span>
<span class="sd">    Useful for inference and memory optimization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_grad_enabled</span>
    <span class="n">old_value</span> <span class="o">=</span> <span class="n">_grad_enabled</span>
    <span class="n">_grad_enabled</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">_grad_enabled</span> <span class="o">=</span> <span class="n">old_value</span></div>



<div class="viewcode-block" id="enable_grad">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.enable_grad">[docs]</a>
<span class="nd">@contextmanager</span>
<span class="k">def</span><span class="w"> </span><span class="nf">enable_grad</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context manager to enable gradient computation.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_grad_enabled</span>
    <span class="n">old_value</span> <span class="o">=</span> <span class="n">_grad_enabled</span>
    <span class="n">_grad_enabled</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">_grad_enabled</span> <span class="o">=</span> <span class="n">old_value</span></div>



<div class="viewcode-block" id="is_grad_enabled">
<a class="viewcode-back" href="../../../api/core.html#neural_arch.core.is_grad_enabled">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">is_grad_enabled</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if gradient computation is currently enabled.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_grad_enabled</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Neural Architecture Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>