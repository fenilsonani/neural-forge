

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optimization Module &mdash; Neural Architecture 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=51b770b3"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Configuration Module" href="config.html" />
    <link rel="prev" title="Neural Network Module" href="nn.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../index.html" class="icon icon-home">
            Neural Architecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="core.html">Core Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="functional.html">Functional Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">Neural Network Module</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimization Module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#optimizers">Optimizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#adam-optimizer">Adam Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.optim.Adam"><code class="docutils literal notranslate"><span class="pre">Adam</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sgd-optimizer">SGD Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.optim.SGD"><code class="docutils literal notranslate"><span class="pre">SGD</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#adamw-optimizer">AdamW Optimizer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.optim.AdamW"><code class="docutils literal notranslate"><span class="pre">AdamW</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#examples">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#basic-usage">Basic Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Adam Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">SGD Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">AdamW Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learning-rate-scheduling">Learning Rate Scheduling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-parameter-groups">Multiple Parameter Groups</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-training-patterns">Advanced Training Patterns</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimizer-state">Optimizer State</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performance-tips">Performance Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="#memory-considerations">Memory Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#implementation-notes">Implementation Notes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="config.html">Configuration Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Module Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../search.html">Search Page</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Neural Architecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Optimization Module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api/optim.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="optimization-module">
<h1>Optimization Module<a class="headerlink" href="#optimization-module" title="Link to this heading"></a></h1>
<p>The optim module provides optimization algorithms for training neural networks.</p>
<section id="optimizers">
<h2>Optimizers<a class="headerlink" href="#optimizers" title="Link to this heading"></a></h2>
<section id="adam-optimizer">
<h3>Adam Optimizer<a class="headerlink" href="#adam-optimizer" title="Link to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.optim.Adam">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.optim.</span></span><span class="sig-name descname"><span class="pre">Adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amsgrad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maximize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adam.html#Adam"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.Adam" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></p>
<p>Adam: A Method for Stochastic Optimization.</p>
<p>Enterprise-grade Adam optimizer with:
- Momentum and RMSprop combination
- Bias correction for early training
- Gradient clipping for stability
- Numerical stability safeguards
- Weight decay support</p>
<p>Reference: <a class="reference external" href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a></p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amsgrad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maximize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adam.html#Adam.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.Adam.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize Adam optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> – Dictionary or iterator of parameters to optimize</p></li>
<li><p><strong>lr</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – Learning rate</p></li>
<li><p><strong>beta1</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – Coefficient for computing running averages of gradient</p></li>
<li><p><strong>beta2</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – Coefficient for computing running averages of squared gradient</p></li>
<li><p><strong>eps</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – Term added to denominator for numerical stability</p></li>
<li><p><strong>weight_decay</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – Weight decay (L2 penalty) coefficient</p></li>
<li><p><strong>amsgrad</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a></span>) – Whether to use AMSGrad variant</p></li>
<li><p><strong>maximize</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a></span>) – Maximize objective instead of minimize</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>OptimizerError</strong> – If parameters are invalid</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_arch.optim.Adam.step" title="Link to this definition"></a></dt>
<dd><p>Perform a single optimization step.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_arch.optim.Adam.zero_grad" title="Link to this definition"></a></dt>
<dd><p>Zero all parameter gradients.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.get_lr">
<span class="sig-name descname"><span class="pre">get_lr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adam.html#Adam.get_lr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.Adam.get_lr" title="Link to this definition"></a></dt>
<dd><p>Get current learning rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.set_lr">
<span class="sig-name descname"><span class="pre">set_lr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lr</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adam.html#Adam.set_lr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.Adam.set_lr" title="Link to this definition"></a></dt>
<dd><p>Set learning rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>lr</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – New learning rate</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>OptimizerError</strong> – If learning rate is invalid</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.get_state_dict">
<span class="sig-name descname"><span class="pre">get_state_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adam.html#Adam.get_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.Adam.get_state_dict" title="Link to this definition"></a></dt>
<dd><p>Get optimizer state dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adam.html#Adam.load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.Adam.load_state_dict" title="Link to this definition"></a></dt>
<dd><p>Load optimizer state dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a></span>) – State dictionary to load</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.__repr__">
<span class="sig-name descname"><span class="pre">__repr__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adam.html#Adam.__repr__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.Adam.__repr__" title="Link to this definition"></a></dt>
<dd><p>String representation of the optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.Adam.get_statistics">
<span class="sig-name descname"><span class="pre">get_statistics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adam.html#Adam.get_statistics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.Adam.get_statistics" title="Link to this definition"></a></dt>
<dd><p>Get optimization statistics for monitoring.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a></span></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Dictionary with optimization statistics</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="sgd-optimizer">
<h3>SGD Optimizer<a class="headerlink" href="#sgd-optimizer" title="Link to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.optim.SGD">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.optim.</span></span><span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/sgd.html#SGD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.SGD" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></p>
<p>Stochastic Gradient Descent optimizer.</p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.SGD.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/sgd.html#SGD.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.SGD.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>, <a class="reference internal" href="core.html#neural_arch.core.Parameter" title="neural_arch.core.base.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>]</span>) – Parameters to optimize (dict or iterator)</p></li>
<li><p><strong>**kwargs</strong> – Optimizer-specific arguments</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.SGD.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_arch.optim.SGD.step" title="Link to this definition"></a></dt>
<dd><p>Perform a single optimization step.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.SGD.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_arch.optim.SGD.zero_grad" title="Link to this definition"></a></dt>
<dd><p>Zero all parameter gradients.</p>
</dd></dl>

</dd></dl>

</section>
<section id="adamw-optimizer">
<h3>AdamW Optimizer<a class="headerlink" href="#adamw-optimizer" title="Link to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.optim.AdamW">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.optim.</span></span><span class="sig-name descname"><span class="pre">AdamW</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adamw.html#AdamW"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.AdamW" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#neural_arch.optim.Adam" title="neural_arch.optim.adam.Adam"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adam</span></code></a></p>
<p>AdamW optimizer (Adam with decoupled weight decay).</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.optim.AdamW.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/optim/adamw.html#AdamW.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.optim.AdamW.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize Adam optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> – Dictionary or iterator of parameters to optimize</p></li>
<li><p><strong>lr</strong> – Learning rate</p></li>
<li><p><strong>beta1</strong> – Coefficient for computing running averages of gradient</p></li>
<li><p><strong>beta2</strong> – Coefficient for computing running averages of squared gradient</p></li>
<li><p><strong>eps</strong> – Term added to denominator for numerical stability</p></li>
<li><p><strong>weight_decay</strong> – Weight decay (L2 penalty) coefficient</p></li>
<li><p><strong>amsgrad</strong> – Whether to use AMSGrad variant</p></li>
<li><p><strong>maximize</strong> – Maximize objective instead of minimize</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>OptimizerError</strong> – If parameters are invalid</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h2>
<section id="basic-usage">
<h3>Basic Usage<a class="headerlink" href="#basic-usage" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">neural_arch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">na</span>

<span class="c1"># Create a simple model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Setup optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Clear gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>Adam Optimizer<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>Adam is the recommended optimizer for most neural network training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adam with default parameters</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Adam with custom parameters</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># Momentum parameter</span>
    <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>    <span class="c1"># RMSprop parameter</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>       <span class="c1"># Numerical stability</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>  <span class="c1"># L2 regularization</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>SGD Optimizer<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<p>Stochastic Gradient Descent with optional momentum:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Basic SGD</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># SGD with momentum</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>AdamW Optimizer<a class="headerlink" href="#id3" title="Link to this heading"></a></h3>
<p>Adam with decoupled weight decay:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># AdamW for better generalization</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>  <span class="c1"># Decoupled weight decay</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="learning-rate-scheduling">
<h3>Learning Rate Scheduling<a class="headerlink" href="#learning-rate-scheduling" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Manual learning rate scheduling</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># Decay learning rate every 30 epochs</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">30</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">lr</span> <span class="o">*=</span> <span class="mf">0.1</span>

    <span class="c1"># Training step</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, LR: </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">lr</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="multiple-parameter-groups">
<h3>Multiple Parameter Groups<a class="headerlink" href="#multiple-parameter-groups" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Different learning rates for different parts</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ComplexModel</span><span class="p">()</span>

<span class="c1"># Get different parameter groups</span>
<span class="n">backbone_params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<span class="n">head_params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>

<span class="c1"># Create optimizer with different learning rates</span>
<span class="c1"># Note: Current implementation uses single parameter dict</span>
<span class="c1"># This is a conceptual example for future enhancement</span>
<span class="n">all_params</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">all_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">backbone_params</span><span class="p">)</span>
<span class="n">all_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">head_params</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">all_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="advanced-training-patterns">
<h3>Advanced Training Patterns<a class="headerlink" href="#advanced-training-patterns" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradient clipping example</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Forward pass</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="p">)</span>

        <span class="c1"># Backward pass</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Optional: Gradient clipping (handled automatically by tensors)</span>
        <span class="c1"># Gradients are clipped during backward pass for numerical stability</span>

        <span class="c1"># Update parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span>

    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Average Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="optimizer-state">
<h3>Optimizer State<a class="headerlink" href="#optimizer-state" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Access optimizer internal state</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Train for some steps</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="c1"># Check optimizer state</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step count: </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Learning rate: </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">lr</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Adam-specific state</span>
<span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Momentum estimates available&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSprop estimates available&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="performance-tips">
<h2>Performance Tips<a class="headerlink" href="#performance-tips" title="Link to this heading"></a></h2>
<dl class="simple">
<dt><strong>Optimizer Choice</strong>:</dt><dd><ul class="simple">
<li><p>Use <strong>Adam</strong> for most applications (adaptive learning rates)</p></li>
<li><p>Use <strong>SGD with momentum</strong> for well-tuned hyperparameters</p></li>
<li><p>Use <strong>AdamW</strong> when regularization is important</p></li>
</ul>
</dd>
<dt><strong>Learning Rate</strong>:</dt><dd><ul class="simple">
<li><p>Start with 0.001 for Adam</p></li>
<li><p>Start with 0.01-0.1 for SGD</p></li>
<li><p>Use learning rate scheduling for better convergence</p></li>
</ul>
</dd>
<dt><strong>Batch Size</strong>:</dt><dd><ul class="simple">
<li><p>Larger batches work better with higher learning rates</p></li>
<li><p>Smaller batches may need more training steps</p></li>
</ul>
</dd>
<dt><strong>Gradient Clipping</strong>:</dt><dd><ul class="simple">
<li><p>Automatically handled by the tensor system</p></li>
<li><p>Prevents gradient explosion in deep networks</p></li>
<li><p>Threshold set to 1e6 for numerical stability</p></li>
</ul>
</dd>
</dl>
</section>
<section id="memory-considerations">
<h2>Memory Considerations<a class="headerlink" href="#memory-considerations" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Parameter Storage</strong>: Optimizers maintain references to model parameters</p></li>
<li><p><strong>State Variables</strong>: Adam stores momentum and RMSprop estimates (doubles memory)</p></li>
<li><p><strong>Gradient Storage</strong>: Gradients are stored in parameter tensors</p></li>
<li><p><strong>Efficient Updates</strong>: In-place parameter updates minimize memory allocation</p></li>
</ul>
</section>
<section id="implementation-notes">
<h2>Implementation Notes<a class="headerlink" href="#implementation-notes" title="Link to this heading"></a></h2>
<p>The optimizers are implemented with:</p>
<ul class="simple">
<li><p><strong>Numerical Stability</strong>: Careful handling of edge cases and numerical precision</p></li>
<li><p><strong>Gradient Clipping</strong>: Automatic clipping of extreme gradients</p></li>
<li><p><strong>Error Handling</strong>: Comprehensive validation of parameters and state</p></li>
<li><p><strong>Performance</strong>: Efficient NumPy operations for parameter updates</p></li>
<li><p><strong>Flexibility</strong>: Support for different parameter types and configurations</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nn.html" class="btn btn-neutral float-left" title="Neural Network Module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="config.html" class="btn btn-neutral float-right" title="Configuration Module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Neural Architecture Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>