

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Neural Network Module &mdash; Neural Architecture 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=51b770b3"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Optimization Module" href="optim.html" />
    <link rel="prev" title="Functional Module" href="functional.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../index.html" class="icon icon-home">
            Neural Architecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="core.html">Core Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="functional.html">Functional Module</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Neural Network Module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#linear-layers">Linear Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.Linear"><code class="docutils literal notranslate"><span class="pre">Linear</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Linear.__init__"><code class="docutils literal notranslate"><span class="pre">Linear.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Linear.forward"><code class="docutils literal notranslate"><span class="pre">Linear.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Linear.reset_parameters"><code class="docutils literal notranslate"><span class="pre">Linear.reset_parameters()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Linear.extra_repr"><code class="docutils literal notranslate"><span class="pre">Linear.extra_repr()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Linear.__repr__"><code class="docutils literal notranslate"><span class="pre">Linear.__repr__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Linear.weight_norm"><code class="docutils literal notranslate"><span class="pre">Linear.weight_norm</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Linear.bias_norm"><code class="docutils literal notranslate"><span class="pre">Linear.bias_norm</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Linear.get_weight_stats"><code class="docutils literal notranslate"><span class="pre">Linear.get_weight_stats()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#embedding-layers">Embedding Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.Embedding"><code class="docutils literal notranslate"><span class="pre">Embedding</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Embedding.__init__"><code class="docutils literal notranslate"><span class="pre">Embedding.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Embedding.forward"><code class="docutils literal notranslate"><span class="pre">Embedding.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Embedding.__call__"><code class="docutils literal notranslate"><span class="pre">Embedding.__call__()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#activation-layers">Activation Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.ReLU"><code class="docutils literal notranslate"><span class="pre">ReLU</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.ReLU.forward"><code class="docutils literal notranslate"><span class="pre">ReLU.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.Softmax"><code class="docutils literal notranslate"><span class="pre">Softmax</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Softmax.__init__"><code class="docutils literal notranslate"><span class="pre">Softmax.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Softmax.forward"><code class="docutils literal notranslate"><span class="pre">Softmax.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.Sigmoid"><code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Sigmoid.forward"><code class="docutils literal notranslate"><span class="pre">Sigmoid.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.Tanh"><code class="docutils literal notranslate"><span class="pre">Tanh</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.Tanh.forward"><code class="docutils literal notranslate"><span class="pre">Tanh.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.GELU"><code class="docutils literal notranslate"><span class="pre">GELU</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.GELU.forward"><code class="docutils literal notranslate"><span class="pre">GELU.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#normalization-layers">Normalization Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.LayerNorm"><code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.LayerNorm.__init__"><code class="docutils literal notranslate"><span class="pre">LayerNorm.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.LayerNorm.forward"><code class="docutils literal notranslate"><span class="pre">LayerNorm.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#attention-mechanisms">Attention Mechanisms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.MultiHeadAttention"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.MultiHeadAttention.__init__"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttention.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.MultiHeadAttention.forward"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttention.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#transformer-components">Transformer Components</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neural_arch.nn.TransformerBlock"><code class="docutils literal notranslate"><span class="pre">TransformerBlock</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.TransformerBlock.__init__"><code class="docutils literal notranslate"><span class="pre">TransformerBlock.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural_arch.nn.TransformerBlock.forward"><code class="docutils literal notranslate"><span class="pre">TransformerBlock.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#examples">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#building-a-simple-neural-network">Building a Simple Neural Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-embedding-layers">Using Embedding Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-head-attention-example">Multi-Head Attention Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-a-transformer-block">Building a Transformer Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="#layer-normalization">Layer Normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-a-neural-network">Training a Neural Network</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#design-patterns">Design Patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performance-considerations">Performance Considerations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimization Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="config.html">Configuration Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Module Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../search.html">Search Page</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Neural Architecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Neural Network Module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api/nn.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="neural-network-module">
<h1>Neural Network Module<a class="headerlink" href="#neural-network-module" title="Link to this heading"></a></h1>
<p>The nn module provides high-level neural network layers and components for building deep learning models.</p>
<section id="linear-layers">
<h2>Linear Layers<a class="headerlink" href="#linear-layers" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.Linear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/linear.html#Linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Linear" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Fully connected (linear) layer with enterprise-grade features.</p>
<p>This layer performs a linear transformation: y = xW + b</p>
<p>Features:
- Multiple weight initialization schemes
- Bias term (optional)
- Gradient tracking and backpropagation
- Memory-efficient implementation
- Comprehensive error handling</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Linear.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/linear.html#Linear.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Linear.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a></span>) – Size of input features</p></li>
<li><p><strong>out_features</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a></span>) – Size of output features</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a></span>) – Whether to include bias term</p></li>
<li><p><strong>weight_init</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a></span>) – Weight initialization scheme</p></li>
<li><p><strong>bias_init</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a></span>) – Bias initialization scheme</p></li>
<li><p><strong>name</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>]</span>) – Optional layer name for debugging</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>LayerError</strong> – If layer parameters are invalid</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Linear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_arch.nn.Linear.forward" title="Link to this definition"></a></dt>
<dd><p>Define the forward computation.</p>
<p>This method must be implemented by all subclasses.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Linear.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/linear.html#Linear.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Linear.reset_parameters" title="Link to this definition"></a></dt>
<dd><p>Reset layer parameters with new initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight_init</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>]</span>) – New weight initialization scheme (keeps current if None)</p></li>
<li><p><strong>bias_init</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>]</span>) – New bias initialization scheme (keeps current if None)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Linear.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/linear.html#Linear.extra_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Linear.extra_repr" title="Link to this definition"></a></dt>
<dd><p>Return extra string representation for debugging.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Linear.__repr__">
<span class="sig-name descname"><span class="pre">__repr__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/linear.html#Linear.__repr__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Linear.__repr__" title="Link to this definition"></a></dt>
<dd><p>String representation of the linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neural_arch.nn.Linear.weight_norm">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">weight_norm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a></em><a class="headerlink" href="#neural_arch.nn.Linear.weight_norm" title="Link to this definition"></a></dt>
<dd><p>Compute the Frobenius norm of the weight matrix.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="neural_arch.nn.Linear.bias_norm">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bias_norm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a></em><a class="headerlink" href="#neural_arch.nn.Linear.bias_norm" title="Link to this definition"></a></dt>
<dd><p>Compute the L2 norm of the bias vector.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Linear.get_weight_stats">
<span class="sig-name descname"><span class="pre">get_weight_stats</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/linear.html#Linear.get_weight_stats"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Linear.get_weight_stats" title="Link to this definition"></a></dt>
<dd><p>Get statistics about layer weights for monitoring.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a></span></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Dictionary with weight statistics</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="embedding-layers">
<h2>Embedding Layers<a class="headerlink" href="#embedding-layers" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.Embedding">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">Embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/embedding.html#Embedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Embedding" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Token embedding layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Embedding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/embedding.html#Embedding.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Embedding.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Embedding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#neural_arch.nn.Embedding.forward" title="Link to this definition"></a></dt>
<dd><p>Define the forward computation.</p>
<p>This method must be implemented by all subclasses.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Embedding.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">indices</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/embedding.html#Embedding.__call__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Embedding.__call__" title="Link to this definition"></a></dt>
<dd><p>Make layer callable.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="activation-layers">
<h2>Activation Layers<a class="headerlink" href="#activation-layers" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.ReLU">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.ReLU" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>ReLU activation layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.ReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#ReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.ReLU.forward" title="Link to this definition"></a></dt>
<dd><p>Define the forward computation.</p>
<p>This method must be implemented by all subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.Softmax">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">Softmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#Softmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Softmax" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Softmax activation layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Softmax.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#Softmax.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Softmax.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Softmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#Softmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Softmax.forward" title="Link to this definition"></a></dt>
<dd><p>Define the forward computation.</p>
<p>This method must be implemented by all subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.Sigmoid">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Sigmoid" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sigmoid activation layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Sigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#Sigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Sigmoid.forward" title="Link to this definition"></a></dt>
<dd><p>Define the forward computation.</p>
<p>This method must be implemented by all subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.Tanh">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">Tanh</span></span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#Tanh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Tanh" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Tanh activation layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.Tanh.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#Tanh.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.Tanh.forward" title="Link to this definition"></a></dt>
<dd><p>Define the forward computation.</p>
<p>This method must be implemented by all subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.GELU">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">GELU</span></span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#GELU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.GELU" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>GELU activation layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.GELU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/activation.html#GELU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.GELU.forward" title="Link to this definition"></a></dt>
<dd><p>Define the forward computation.</p>
<p>This method must be implemented by all subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="normalization-layers">
<h2>Normalization Layers<a class="headerlink" href="#normalization-layers" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.LayerNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/normalization.html#LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.LayerNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Layer normalization.</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.LayerNorm.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/normalization.html#LayerNorm.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.LayerNorm.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.LayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/normalization.html#LayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.LayerNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="attention-mechanisms">
<h2>Attention Mechanisms<a class="headerlink" href="#attention-mechanisms" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.MultiHeadAttention">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">MultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/attention.html#MultiHeadAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.MultiHeadAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Multi-head attention mechanism.</p>
<p>This implements the attention mechanism from “Attention Is All You Need”
with enterprise-grade features:
- Scaled dot-product attention
- Multiple attention heads
- Residual connections and layer normalization
- Efficient batched computation
- Gradient tracking and backpropagation</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.MultiHeadAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/attention.html#MultiHeadAttention.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.MultiHeadAttention.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize multi-head attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a></span>) – Model dimension</p></li>
<li><p><strong>num_heads</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a></span>) – Number of attention heads</p></li>
<li><p><strong>dropout</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – Dropout probability (not implemented yet)</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a></span>) – Whether to use bias in linear layers</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.MultiHeadAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/attention.html#MultiHeadAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.MultiHeadAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass for multi-head attention.</p>
<p>Simplified implementation that maintains gradient flow by using a
single transformation that approximates multi-head attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span>) – Input tensor of shape (batch_size, seq_len, d_model)</p></li>
<li><p><strong>mask</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]</span>) – Optional attention mask</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Output tensor of shape (batch_size, seq_len, d_model)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="transformer-components">
<h2>Transformer Components<a class="headerlink" href="#transformer-components" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neural_arch.nn.TransformerBlock">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_arch.nn.</span></span><span class="sig-name descname"><span class="pre">TransformerBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/transformer.html#TransformerBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.TransformerBlock" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="core.html#neural_arch.core.Module" title="neural_arch.core.base.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Transformer block (placeholder).</p>
<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.TransformerBlock.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/transformer.html#TransformerBlock.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.TransformerBlock.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neural_arch.nn.TransformerBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/neural_arch/nn/transformer.html#TransformerBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_arch.nn.TransformerBlock.forward" title="Link to this definition"></a></dt>
<dd><p>Define the forward computation.</p>
<p>This method must be implemented by all subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="core.html#neural_arch.core.Tensor" title="neural_arch.core.tensor.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h2>
<section id="building-a-simple-neural-network">
<h3>Building a Simple Neural Network<a class="headerlink" href="#building-a-simple-neural-network" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">neural_arch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">na</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SimpleClassifier</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">params</span>

<span class="c1"># Create and use the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleClassifier</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-embedding-layers">
<h3>Using Embedding Layers<a class="headerlink" href="#using-embedding-layers" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># For text processing</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

<span class="c1"># Input: batch of token sequences</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>  <span class="c1"># Shape: (2, 4)</span>
<span class="n">embedded</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>  <span class="c1"># Shape: (2, 4, 300)</span>
</pre></div>
</div>
</section>
<section id="multi-head-attention-example">
<h3>Multi-Head Attention Example<a class="headerlink" href="#multi-head-attention-example" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Transformer-style attention</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">attention</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1"># Input: (batch_size, sequence_length, d_model)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">attended</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Same shape as input</span>
</pre></div>
</div>
</section>
<section id="building-a-transformer-block">
<h3>Building a Transformer Block<a class="headerlink" href="#building-a-transformer-block" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Complete transformer block</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">2048</span>

<span class="n">transformer_block</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>

<span class="c1"># Process sequence</span>
<span class="n">sequence</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="layer-normalization">
<h3>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Normalize features</span>
<span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">512</span>

<span class="n">layer_norm</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Normalize along the last dimension</span>
<span class="n">normalized</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Check normalization properties</span>
<span class="n">mean_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">normalized</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Should be ~0</span>
<span class="n">std_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">normalized</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># Should be ~1</span>
</pre></div>
</div>
</section>
<section id="training-a-neural-network">
<h3>Training a Neural Network<a class="headerlink" href="#training-a-neural-network" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Complete training example</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">neural_arch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">na</span>

<span class="c1"># Define model</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">na</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">na</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">na</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">na</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">na</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="n">na</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
        <span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;parameters&#39;</span><span class="p">):</span>
                <span class="n">layer_params</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">layer_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;layer_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
        <span class="k">return</span> <span class="n">params</span>

<span class="c1"># Training setup</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">na</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="c1"># Backward pass</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="design-patterns">
<h2>Design Patterns<a class="headerlink" href="#design-patterns" title="Link to this heading"></a></h2>
<p><strong>Module Pattern</strong>: All neural network components inherit from the base Module class, providing consistent interfaces for parameters and computation.</p>
<p><strong>Functional Core</strong>: Layers are thin wrappers around functional operations, maintaining separation between computation and state.</p>
<p><strong>Gradient Flow</strong>: All layers are designed to properly propagate gradients through the computational graph.</p>
<p><strong>Parameter Management</strong>: Consistent parameter naming and access patterns across all layers.</p>
</section>
<section id="performance-considerations">
<h2>Performance Considerations<a class="headerlink" href="#performance-considerations" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Memory Efficiency</strong>: Layers minimize memory allocations during forward and backward passes</p></li>
<li><p><strong>Gradient Computation</strong>: Efficient backpropagation through all layer types</p></li>
<li><p><strong>Numerical Stability</strong>: Careful implementation of normalization and activation functions</p></li>
<li><p><strong>Batch Processing</strong>: All layers support batch processing for efficient training</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="functional.html" class="btn btn-neutral float-left" title="Functional Module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="optim.html" class="btn btn-neutral float-right" title="Optimization Module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Neural Architecture Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>