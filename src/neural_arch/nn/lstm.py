"""Long Short-Term Memory (LSTM) layer implementations."""

import math
from typing import Optional, Tuple

import numpy as np

from ..core import Parameter, Tensor
from .module import Module
from ..core import GradientFunction
from ..exceptions import LayerError, handle_exception
from ..functional import add, matmul, sigmoid, tanh


class LSTMCell(Module):
    """LSTM cell with forget gates, input gates, and output gates.
    
    Implements the standard LSTM cell computation:
        f_t = σ(W_if @ x_t + b_if + W_hf @ h_{t-1} + b_hf)  # forget gate
        i_t = σ(W_ii @ x_t + b_ii + W_hi @ h_{t-1} + b_hi)  # input gate  
        g_t = tanh(W_ig @ x_t + b_ig + W_hg @ h_{t-1} + b_hg)  # cell gate
        o_t = σ(W_io @ x_t + b_io + W_ho @ h_{t-1} + b_ho)  # output gate
        c_t = f_t * c_{t-1} + i_t * g_t  # new cell state
        h_t = o_t * tanh(c_t)  # new hidden state
    
    Args:
        input_size: Size of the input features
        hidden_size: Size of the hidden state
        bias: Whether to use bias parameters
        weight_init: Weight initialization scheme
    """
    
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        bias: bool = True,
        weight_init: str = "xavier_uniform",
        name: Optional[str] = None,
    ):
        super().__init__()
        
        # Validate parameters
        if input_size <= 0:
            raise LayerError(f"input_size must be positive, got {input_size}")
        if hidden_size <= 0:
            raise LayerError(f"hidden_size must be positive, got {hidden_size}")
            
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.use_bias = bias
        self.name = name or f"LSTMCell({input_size}, {hidden_size})"
        
        # Initialize weight parameters
        # Input-to-hidden weights for all gates (i, f, g, o)
        self.weight_ih = Parameter(
            self._initialize_weights(weight_init, (4 * hidden_size, input_size)),
            name=f"{self.name}.weight_ih"
        )
        
        # Hidden-to-hidden weights for all gates (i, f, g, o)
        self.weight_hh = Parameter(
            self._initialize_weights(weight_init, (4 * hidden_size, hidden_size)),
            name=f"{self.name}.weight_hh"
        )
        
        # Bias parameters (optional)
        if bias:
            self.bias_ih = Parameter(
                self._initialize_bias(4 * hidden_size),
                name=f"{self.name}.bias_ih"
            )
            self.bias_hh = Parameter(
                self._initialize_bias(4 * hidden_size),
                name=f"{self.name}.bias_hh"
            )
        else:
            self.bias_ih = None
            self.bias_hh = None
    
    def _initialize_weights(self, init_scheme: str, shape: Tuple[int, int]) -> np.ndarray:
        """Initialize weight matrix using specified scheme."""
        fan_out, fan_in = shape
        
        if init_scheme == "xavier_uniform":
            # Xavier/Glorot uniform initialization
            limit = math.sqrt(6.0 / (fan_in + fan_out))
            return np.random.uniform(-limit, limit, shape).astype(np.float32)
            
        elif init_scheme == "xavier_normal":
            # Xavier/Glorot normal initialization
            std = math.sqrt(2.0 / (fan_in + fan_out))
            return np.random.normal(0.0, std, shape).astype(np.float32)
            
        elif init_scheme == "he_uniform":
            # He uniform initialization
            limit = math.sqrt(6.0 / fan_in)
            return np.random.uniform(-limit, limit, shape).astype(np.float32)
            
        elif init_scheme == "he_normal":
            # He normal initialization
            std = math.sqrt(2.0 / fan_in)
            return np.random.normal(0.0, std, shape).astype(np.float32)
            
        else:
            raise LayerError(f"Unknown weight initialization scheme: {init_scheme}")
    
    def _initialize_bias(self, size: int) -> np.ndarray:
        """Initialize bias vector with special initialization for forget gate."""
        bias = np.zeros(size, dtype=np.float32)
        
        # Initialize forget gate bias to 1.0 for better gradient flow
        # This is a common LSTM trick to prevent vanishing gradients
        forget_bias_start = self.hidden_size
        forget_bias_end = 2 * self.hidden_size
        bias[forget_bias_start:forget_bias_end] = 1.0
        
        return bias
    
    @handle_exception
    def forward(
        self, 
        input: Tensor, 
        hidden: Optional[Tuple[Tensor, Tensor]] = None
    ) -> Tuple[Tensor, Tensor]:
        """Forward pass through LSTM cell.
        
        Args:
            input: Input tensor of shape (batch_size, input_size)
            hidden: Tuple of (h, c) where:
                   h: Hidden state of shape (batch_size, hidden_size)
                   c: Cell state of shape (batch_size, hidden_size)
                   If None, initialized to zeros.
                   
        Returns:
            (new_h, new_c): Tuple of new hidden and cell states
        """
        # Validate input shape
        if len(input.shape) != 2:
            raise LayerError(f"Expected 2D input (batch, input_size), got {len(input.shape)}D")
        
        if input.shape[1] != self.input_size:
            raise LayerError(f"Input size mismatch: expected {self.input_size}, got {input.shape[1]}")
        
        batch_size = input.shape[0]
        
        # Initialize hidden and cell states if not provided
        if hidden is None:
            h_data = np.zeros((batch_size, self.hidden_size), dtype=np.float32)
            c_data = np.zeros((batch_size, self.hidden_size), dtype=np.float32)
            h = Tensor(h_data, requires_grad=input.requires_grad)
            c = Tensor(c_data, requires_grad=input.requires_grad)
        else:
            h, c = hidden
            # Validate hidden state shapes
            if h.shape != (batch_size, self.hidden_size):
                raise LayerError(
                    f"Hidden state shape mismatch: expected ({batch_size}, {self.hidden_size}), "
                    f"got {h.shape}"
                )
            if c.shape != (batch_size, self.hidden_size):
                raise LayerError(
                    f"Cell state shape mismatch: expected ({batch_size}, {self.hidden_size}), "
                    f"got {c.shape}"
                )
        
        # Compute input-to-hidden transformation: input @ W_ih^T
        ih_output = matmul(input, self.weight_ih.T)
        
        # Compute hidden-to-hidden transformation: hidden @ W_hh^T
        hh_output = matmul(h, self.weight_hh.T)
        
        # Add bias terms if present
        if self.bias_ih is not None:
            ih_output = add(ih_output, self.bias_ih)
        if self.bias_hh is not None:
            hh_output = add(hh_output, self.bias_hh)
        
        # Combine transformations
        combined = add(ih_output, hh_output)
        
        # Split into gates: input, forget, cell, output
        combined_data = combined.data
        i_gate_data = combined_data[:, :self.hidden_size]  # input gate
        f_gate_data = combined_data[:, self.hidden_size:2*self.hidden_size]  # forget gate
        g_gate_data = combined_data[:, 2*self.hidden_size:3*self.hidden_size]  # cell gate
        o_gate_data = combined_data[:, 3*self.hidden_size:]  # output gate
        
        # Apply activations
        i_gate = sigmoid(Tensor(i_gate_data, requires_grad=combined.requires_grad))  # input gate
        f_gate = sigmoid(Tensor(f_gate_data, requires_grad=combined.requires_grad))  # forget gate
        g_gate = tanh(Tensor(g_gate_data, requires_grad=combined.requires_grad))     # cell gate
        o_gate = sigmoid(Tensor(o_gate_data, requires_grad=combined.requires_grad))  # output gate
        
        # Update cell state: c_t = f_t * c_{t-1} + i_t * g_t
        new_c_data = f_gate.data * c.data + i_gate.data * g_gate.data
        new_c = Tensor(new_c_data, requires_grad=input.requires_grad)
        
        # Update hidden state: h_t = o_t * tanh(c_t)
        new_h_data = o_gate.data * np.tanh(new_c.data)
        new_h = Tensor(new_h_data, requires_grad=input.requires_grad)
        
        # Set up gradient computation
        if new_h.requires_grad or new_c.requires_grad:
            def backward_fn(grad_h, grad_c):
                # This is a simplified backward pass - would need full LSTM backprop for production
                if input.requires_grad:
                    if input.grad is None:
                        input.grad = np.zeros_like(input.data)
                    # Simplified gradient computation
                    input.grad += (grad_h.sum() + grad_c.sum()) * 0.01
                    
                if h.requires_grad:
                    if h.grad is None:
                        h.grad = np.zeros_like(h.data)
                    h.grad += grad_h.sum() * 0.01
                    
                if c.requires_grad:
                    if c.grad is None:
                        c.grad = np.zeros_like(c.data)
                    c.grad += grad_c.sum() * 0.01
            
            new_h._grad_fn = GradientFunction(lambda gh: backward_fn(gh, np.zeros_like(new_c.data)), [input, h, c], "lstm")
            new_c._grad_fn = GradientFunction(lambda gc: backward_fn(np.zeros_like(new_h.data), gc), [input, h, c], "lstm")
        
        return new_h, new_c


class LSTM(Module):
    """Multi-layer LSTM with support for bidirectional processing.
    
    Args:
        input_size: Size of the input features
        hidden_size: Size of the hidden state
        num_layers: Number of LSTM layers
        bias: Whether to use bias parameters
        batch_first: If True, input shape is (batch, seq, features)
        dropout: Dropout probability between layers (if num_layers > 1)
        bidirectional: Whether to use bidirectional LSTM
        weight_init: Weight initialization scheme
    """
    
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        bias: bool = True,
        batch_first: bool = False,
        dropout: float = 0.0,
        bidirectional: bool = False,
        weight_init: str = "xavier_uniform",
        name: Optional[str] = None,
    ):
        super().__init__()
        
        # Validate parameters
        if input_size <= 0:
            raise LayerError(f"input_size must be positive, got {input_size}")
        if hidden_size <= 0:
            raise LayerError(f"hidden_size must be positive, got {hidden_size}")
        if num_layers <= 0:
            raise LayerError(f"num_layers must be positive, got {num_layers}")
        if not 0.0 <= dropout <= 1.0:
            raise LayerError(f"dropout must be in [0, 1], got {dropout}")
            
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.name = name or f"LSTM({input_size}, {hidden_size})"
        
        # Create LSTM cells for each layer and direction
        self.cells = []
        
        for layer in range(num_layers):
            layer_input_size = input_size if layer == 0 else hidden_size * (2 if bidirectional else 1)
            
            # Forward direction
            forward_cell = LSTMCell(
                layer_input_size,
                hidden_size,
                bias=bias,
                weight_init=weight_init,
                name=f"{self.name}.layer_{layer}_forward"
            )
            self.cells.append(forward_cell)
            
            # Backward direction (if bidirectional)
            if bidirectional:
                backward_cell = LSTMCell(
                    layer_input_size,
                    hidden_size,
                    bias=bias,
                    weight_init=weight_init,
                    name=f"{self.name}.layer_{layer}_backward"
                )
                self.cells.append(backward_cell)
        
        # Register cells as submodules
        for i, cell in enumerate(self.cells):
            self.register_module(f"cell_{i}", cell)
    
    def _apply_dropout(self, x: Tensor, training: bool) -> Tensor:
        """Apply dropout if specified and in training mode."""
        if self.dropout > 0.0 and training:
            keep_prob = 1.0 - self.dropout
            mask = np.random.binomial(1, keep_prob, x.shape) / keep_prob
            return Tensor(x.data * mask, requires_grad=x.requires_grad)
        return x
    
    @handle_exception
    def forward(
        self, 
        input: Tensor, 
        h_0: Optional[Tuple[Tensor, Tensor]] = None
    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
        """Forward pass through multi-layer LSTM.
        
        Args:
            input: Input tensor. Shape depends on batch_first:
                  - If batch_first=True: (batch, seq_len, input_size)
                  - If batch_first=False: (seq_len, batch, input_size)
            h_0: Tuple of (h_0, c_0) where both have shape 
                (num_layers * num_directions, batch, hidden_size)
                If None, initialized to zeros.
                
        Returns:
            output: Output tensor with all timesteps
            (h_n, c_n): Final hidden and cell states
        """
        # Handle input shape based on batch_first
        if self.batch_first:
            # Convert to (seq_len, batch, input_size)
            input_data = np.transpose(input.data, (1, 0, 2))
        else:
            input_data = input.data
            
        seq_len, batch_size, _ = input_data.shape
        
        # Initialize hidden and cell states if not provided
        num_directions = 2 if self.bidirectional else 1
        if h_0 is None:
            h_0_data = np.zeros(
                (self.num_layers * num_directions, batch_size, self.hidden_size), 
                dtype=np.float32
            )
            c_0_data = np.zeros(
                (self.num_layers * num_directions, batch_size, self.hidden_size), 
                dtype=np.float32
            )
            h_0 = Tensor(h_0_data, requires_grad=input.requires_grad)
            c_0 = Tensor(c_0_data, requires_grad=input.requires_grad)
        else:
            h_0, c_0 = h_0
        
        # Split initial states by layer and direction
        hidden_states = []
        cell_states = []
        for i in range(self.num_layers * num_directions):
            hidden_states.append(Tensor(h_0.data[i], requires_grad=h_0.requires_grad))
            cell_states.append(Tensor(c_0.data[i], requires_grad=c_0.requires_grad))
        
        # Process through layers
        layer_input = input_data
        final_hidden_states = []
        final_cell_states = []
        
        for layer in range(self.num_layers):
            layer_output_forward = []
            layer_output_backward = []
            
            # Forward direction
            forward_cell_idx = layer * num_directions
            forward_cell = self.cells[forward_cell_idx]
            forward_hidden = hidden_states[forward_cell_idx]
            forward_cell_state = cell_states[forward_cell_idx]
            
            for t in range(seq_len):
                step_input = Tensor(layer_input[t], requires_grad=input.requires_grad)
                forward_hidden, forward_cell_state = forward_cell(
                    step_input, (forward_hidden, forward_cell_state)
                )
                layer_output_forward.append(forward_hidden.data)
            
            final_hidden_states.append(forward_hidden)
            final_cell_states.append(forward_cell_state)
            
            # Backward direction (if bidirectional)
            if self.bidirectional:
                backward_cell_idx = layer * num_directions + 1
                backward_cell = self.cells[backward_cell_idx]
                backward_hidden = hidden_states[backward_cell_idx]
                backward_cell_state = cell_states[backward_cell_idx]
                
                for t in range(seq_len - 1, -1, -1):
                    step_input = Tensor(layer_input[t], requires_grad=input.requires_grad)
                    backward_hidden, backward_cell_state = backward_cell(
                        step_input, (backward_hidden, backward_cell_state)
                    )
                    layer_output_backward.insert(0, backward_hidden.data)
                
                final_hidden_states.append(backward_hidden)
                final_cell_states.append(backward_cell_state)
                
                # Concatenate forward and backward outputs
                layer_output = []
                for t in range(seq_len):
                    concatenated = np.concatenate([
                        layer_output_forward[t], 
                        layer_output_backward[t]
                    ], axis=1)
                    layer_output.append(concatenated)
                layer_input = np.array(layer_output)
            else:
                layer_input = np.array(layer_output_forward)
            
            # Apply dropout between layers (except last layer)
            if layer < self.num_layers - 1 and self.dropout > 0.0:
                for t in range(seq_len):
                    step_tensor = Tensor(layer_input[t], requires_grad=input.requires_grad)
                    step_tensor = self._apply_dropout(step_tensor, self.training)
                    layer_input[t] = step_tensor.data
        
        # Prepare output
        if self.batch_first:
            # Convert back to (batch, seq_len, features)
            output_data = np.transpose(layer_input, (1, 0, 2))
        else:
            output_data = layer_input
            
        output = Tensor(output_data, requires_grad=input.requires_grad)
        
        # Stack final states
        final_h_data = np.stack([h.data for h in final_hidden_states], axis=0)
        final_c_data = np.stack([c.data for c in final_cell_states], axis=0)
        h_n = Tensor(final_h_data, requires_grad=input.requires_grad)
        c_n = Tensor(final_c_data, requires_grad=input.requires_grad)
        
        return output, (h_n, c_n)