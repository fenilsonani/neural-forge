"""Demonstration of Differential Attention - Latest research from October 2024.

This demo shows the key benefits of Differential Attention:
1. Noise cancellation through attention subtraction
2. Emergence of sparse attention patterns
3. Better focus on relevant context
4. Reduced hallucination in language models

Run with: python examples/differential_attention_demo.py
"""

import numpy as np
import matplotlib.pyplot as plt

from neural_arch.core import Tensor
from neural_arch.nn.differential_attention import (
    DifferentialAttention,
    DifferentialTransformerBlock
)
from neural_arch.nn.attention import MultiHeadAttention


def visualize_attention_patterns():
    """Visualize attention patterns: Standard vs Differential."""
    print("\n" + "="*60)
    print("ATTENTION PATTERN VISUALIZATION")
    print("="*60)
    
    # Create input with structure
    seq_len = 32
    d_model = 128
    batch_size = 1
    
    # Create input with some patterns
    # First half: relevant signal
    # Second half: noise/distraction
    x_signal = np.random.randn(batch_size, seq_len//2, d_model) * 2.0
    x_noise = np.random.randn(batch_size, seq_len//2, d_model) * 0.5
    x_data = np.concatenate([x_signal, x_noise], axis=1)
    x = Tensor(x_data)
    
    # Differential Attention
    diff_attn = DifferentialAttention(d_model, n_heads=8, lambda_init=0.5)
    output_diff, (attn1, attn2) = diff_attn(x, return_attention_weights=True)
    
    # Compute effective differential attention
    lambda_val = diff_attn.lambda_param.data.mean()
    diff_weights = (1 + lambda_val) * attn1 - lambda_val * attn2
    
    # Average across heads and batch
    attn1_avg = attn1.mean(axis=(0, 1))
    attn2_avg = attn2.mean(axis=(0, 1))
    diff_weights_avg = diff_weights.mean(axis=(0, 1))
    
    # Create visualization
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Plot Attention Map 1
    im1 = axes[0].imshow(attn1_avg, cmap='hot', aspect='auto')
    axes[0].set_title('Attention Map 1 (Standard-like)')
    axes[0].set_xlabel('Key Position')
    axes[0].set_ylabel('Query Position')
    plt.colorbar(im1, ax=axes[0])
    
    # Plot Attention Map 2
    im2 = axes[1].imshow(attn2_avg, cmap='hot', aspect='auto')
    axes[1].set_title('Attention Map 2 (Noise Pattern)')
    axes[1].set_xlabel('Key Position')
    axes[1].set_ylabel('Query Position')
    plt.colorbar(im2, ax=axes[1])
    
    # Plot Differential Attention
    im3 = axes[2].imshow(diff_weights_avg, cmap='RdBu_r', aspect='auto', vmin=-0.1, vmax=0.1)
    axes[2].set_title('Differential Attention (Noise Canceled)')
    axes[2].set_xlabel('Key Position')
    axes[2].set_ylabel('Query Position')
    plt.colorbar(im3, ax=axes[2])
    
    plt.tight_layout()
    plt.savefig('differential_attention_patterns.png', dpi=150)
    print("Saved visualization to differential_attention_patterns.png")
    
    # Calculate sparsity metrics
    threshold = 0.01
    sparsity_attn1 = np.mean(attn1_avg < threshold)
    sparsity_diff = np.mean(np.abs(diff_weights_avg) < threshold)
    
    print(f"\nSparsity Analysis:")
    print(f"  Standard Attention Sparsity: {sparsity_attn1:.1%}")
    print(f"  Differential Attention Sparsity: {sparsity_diff:.1%}")
    print(f"  Improvement: {(sparsity_diff/sparsity_attn1 - 1)*100:.1f}% more sparse")


def demonstrate_noise_cancellation():
    """Demonstrate how differential attention cancels noise."""
    print("\n" + "="*60)
    print("NOISE CANCELLATION DEMONSTRATION")
    print("="*60)
    
    batch_size = 2
    seq_len = 16
    d_model = 64
    
    # Create input with clear signal + noise
    signal = np.sin(np.linspace(0, 2*np.pi, seq_len)).reshape(1, seq_len, 1)
    signal = np.tile(signal, (batch_size, 1, d_model))
    
    noise = np.random.randn(batch_size, seq_len, d_model) * 0.5
    
    x_noisy = Tensor(signal + noise)
    x_clean = Tensor(signal)
    
    # Process through Differential Attention
    diff_attn = DifferentialAttention(d_model, n_heads=4)
    
    output_noisy = diff_attn(x_noisy)
    output_clean = diff_attn(x_clean)
    
    # Measure signal preservation
    signal_correlation_noisy = np.corrcoef(
        output_noisy.data.flatten(),
        signal.flatten()
    )[0, 1]
    
    signal_correlation_clean = np.corrcoef(
        output_clean.data.flatten(),
        signal.flatten()
    )[0, 1]
    
    print(f"\nSignal Preservation:")
    print(f"  Clean input correlation: {signal_correlation_clean:.3f}")
    print(f"  Noisy input correlation: {signal_correlation_noisy:.3f}")
    print(f"  Noise resistance: {(signal_correlation_noisy/signal_correlation_clean)*100:.1f}%")


def compare_attention_mechanisms():
    """Compare Differential Attention with standard Multi-Head Attention."""
    print("\n" + "="*60)
    print("DIFFERENTIAL vs STANDARD ATTENTION COMPARISON")
    print("="*60)
    
    # Test configuration
    batch_size = 4
    seq_len = 64
    d_model = 256
    n_heads = 8
    
    x = Tensor(np.random.randn(batch_size, seq_len, d_model))
    
    # Standard Multi-Head Attention
    mha = MultiHeadAttention(d_model, n_heads)
    output_mha = mha(x)
    
    # Differential Attention
    diff_attn = DifferentialAttention(d_model, n_heads)
    output_diff = diff_attn(x)
    
    # Compare outputs
    print(f"\nOutput Statistics:")
    print(f"  MHA output mean: {output_mha.data.mean():.4f}")
    print(f"  MHA output std: {output_mha.data.std():.4f}")
    print(f"  Diff output mean: {output_diff.data.mean():.4f}")
    print(f"  Diff output std: {output_diff.data.std():.4f}")
    
    # Lambda statistics
    lambda_stats = diff_attn.get_attention_stats()
    print(f"\nLearned Lambda Parameters:")
    print(f"  Mean: {lambda_stats['lambda_mean']:.3f}")
    print(f"  Std: {lambda_stats['lambda_std']:.3f}")
    print(f"  Range: [{lambda_stats['lambda_min']:.3f}, {lambda_stats['lambda_max']:.3f}]")


def test_long_context_improvement():
    """Test improvement on long-context scenarios."""
    print("\n" + "="*60)
    print("LONG-CONTEXT HANDLING TEST")
    print("="*60)
    
    # Simulate long context with important info at beginning and end
    batch_size = 2
    seq_len = 128
    d_model = 128
    
    # Create input with important info at positions 0-10 and 118-128
    # Middle is filled with distractors
    important_start = np.random.randn(batch_size, 10, d_model) * 2.0
    important_end = np.random.randn(batch_size, 10, d_model) * 2.0
    distractor = np.random.randn(batch_size, seq_len - 20, d_model) * 0.3
    
    x_data = np.concatenate([important_start, distractor, important_end], axis=1)
    x = Tensor(x_data)
    
    # Process through Differential Attention
    diff_attn = DifferentialAttention(d_model, n_heads=8, lambda_init=0.5)
    output, (attn1, attn2) = diff_attn(x, return_attention_weights=True)
    
    # Analyze attention to important vs distractor regions
    lambda_val = diff_attn.lambda_param.data.mean()
    diff_weights = (1 + lambda_val) * attn1 - lambda_val * attn2
    
    # Average attention weights
    avg_attention = diff_weights.mean(axis=(0, 1))  # Average over batch and heads
    
    # Attention to important regions vs distractors
    attn_to_start = avg_attention[:, :10].mean()
    attn_to_end = avg_attention[:, -10:].mean()
    attn_to_middle = avg_attention[:, 10:-10].mean()
    
    print(f"\nAttention Distribution:")
    print(f"  To important start: {attn_to_start:.4f}")
    print(f"  To important end: {attn_to_end:.4f}")
    print(f"  To distractors (middle): {attn_to_middle:.4f}")
    
    focus_ratio = (attn_to_start + attn_to_end) / (2 * attn_to_middle)
    print(f"\nFocus Ratio (important/distractor): {focus_ratio:.2f}x")
    print(f"â†’ Higher ratio means better focus on relevant context")


def main():
    """Run all demonstrations."""
    print("\n" + "ðŸ”¬ "*20)
    print("DIFFERENTIAL ATTENTION DEMONSTRATION")
    print("Implementing: 'Differential Transformer' (arXiv:2410.05258)")
    print("Published: October 2024 | Microsoft Research")
    print("ðŸ”¬ "*20)
    
    # Run demonstrations
    compare_attention_mechanisms()
    demonstrate_noise_cancellation()
    test_long_context_improvement()
    
    # Optional: visualize if matplotlib is available
    try:
        visualize_attention_patterns()
    except ImportError:
        print("\nSkipping visualization (matplotlib not installed)")
    
    print("\n" + "="*60)
    print("KEY BENEFITS OF DIFFERENTIAL ATTENTION")
    print("="*60)
    print("""
âœ… Noise Cancellation: Subtracts attention maps to remove common noise
âœ… Sparse Patterns: Promotes focused attention on relevant content
âœ… Hallucination Reduction: 50% reduction shown in paper
âœ… Long Context: Better handling of long sequences
âœ… Interpretability: Clearer attention patterns

This implementation demonstrates cutting-edge research from just 3 months ago!
The Differential Transformer represents the future of attention mechanisms.
    """)


if __name__ == "__main__":
    main()