#!/usr/bin/env python3
"""
üöÄ Neural Architecture Framework - Comprehensive Streamlit Demo

Interactive web interface showcasing all advanced model architectures with:
- BERT bidirectional text understanding
- GPT-2 autoregressive text generation  
- Vision Transformer image classification
- Modern Transformer with RoPE and advanced features
- CLIP multimodal vision-language understanding
- ResNet computer vision with residual learning
- Real-time performance metrics and visualizations
- Interactive model comparison and benchmarking
"""

import streamlit as st
import sys
import os
import numpy as np
import time
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List, Any, Optional
import json

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# Import all model architectures
from neural_arch.core import Tensor
from neural_arch.models.language.bert import BERTConfig, BERT
from neural_arch.models.language.gpt2 import GPT2_CONFIGS, GPT2LMHead
from neural_arch.models.vision.vision_transformer import VisionTransformer
from neural_arch.models.language.modern_transformer import PreNormTransformer, PreNormTransformerConfig
from neural_arch.models.multimodal.clip import CLIP, CLIP_CONFIGS
from neural_arch.models.vision.resnet import ResNet18, ResNet34, ResNet50
from neural_arch.optim import AdamW
from neural_arch.functional import softmax, cross_entropy_loss
from neural_arch.optimization_config import configure, get_config
from neural_arch.backends import available_backends

# Page configuration
st.set_page_config(
    page_title="üöÄ Neural Architecture Framework Demo",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .model-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        color: white;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
    }
    .performance-metric {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .optimization-badge {
        background: #28a745;
        color: white;
        padding: 0.3rem 0.8rem;
        border-radius: 20px;
        font-size: 0.8rem;
        margin: 0.2rem;
        display: inline-block;
    }
    .architecture-comparison {
        background: #ffffff;
        border: 2px solid #e9ecef;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Main title
st.markdown('<h1 class="main-header">üöÄ Neural Architecture Framework</h1>', unsafe_allow_html=True)
st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">Interactive Showcase of Advanced AI Architectures with Automatic Optimizations</p>', unsafe_allow_html=True)

# Sidebar for model selection
st.sidebar.title("üéõÔ∏è Model Selection")
selected_model = st.sidebar.selectbox(
    "Choose Architecture:",
    ["üè† Overview", "üß† BERT", "üé≠ GPT-2", "üñºÔ∏è Vision Transformer", "üß¨ Modern Transformer", "üåü CLIP Multimodal", "üèóÔ∏è ResNet", "üìä Model Comparison"]
)

# Global optimization settings
st.sidebar.title("‚öôÔ∏è Optimization Settings")
enable_optimizations = st.sidebar.checkbox("Enable Automatic Optimizations", value=True)
enable_fusion = st.sidebar.checkbox("Operator Fusion", value=True)
enable_jit = st.sidebar.checkbox("JIT Compilation", value=True)
auto_backend = st.sidebar.checkbox("Auto Backend Selection", value=True)

if enable_optimizations:
    configure(
        enable_fusion=enable_fusion,
        enable_jit=enable_jit,
        auto_backend_selection=auto_backend,
        enable_mixed_precision=False
    )

# Helper functions
@st.cache_data
def create_performance_chart(models_data: List[Dict]) -> go.Figure:
    """Create performance comparison chart."""
    fig = go.Figure()
    
    model_names = [model['name'] for model in models_data]
    parameters = [model['parameters'] for model in models_data]
    inference_times = [model['inference_time'] for model in models_data]
    
    fig.add_trace(go.Scatter(
        x=parameters,
        y=inference_times,
        mode='markers+text',
        text=model_names,
        textposition="top center",
        marker=dict(size=15, color='rgba(31, 119, 180, 0.8)'),
        name='Models'
    ))
    
    fig.update_layout(
        title="Model Performance: Parameters vs Inference Time",
        xaxis_title="Parameters (millions)",
        yaxis_title="Inference Time (seconds)",
        height=500,
        template="plotly_white"
    )
    
    return fig

def create_optimization_metrics() -> Dict[str, Any]:
    """Show current optimization settings."""
    config = get_config()
    return {
        'fusion_enabled': config.optimization.enable_fusion,
        'jit_enabled': config.optimization.enable_jit,
        'auto_backend': config.optimization.auto_backend_selection,
        'available_backends': available_backends()
    }

def display_model_card(title: str, description: str, features: List[str], parameters: int, performance: Dict[str, float]):
    """Display a model information card."""
    st.markdown(f'<div class="model-card">', unsafe_allow_html=True)
    st.markdown(f"### {title}")
    st.markdown(f"**{description}**")
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**üîß Key Features:**")
        for feature in features:
            st.markdown(f"‚Ä¢ {feature}")
    
    with col2:
        st.markdown("**üìä Performance:**")
        st.markdown(f"‚Ä¢ Parameters: {parameters:,}")
        for key, value in performance.items():
            if isinstance(value, float):
                st.markdown(f"‚Ä¢ {key}: {value:.4f}s")
            else:
                st.markdown(f"‚Ä¢ {key}: {value}")
    
    st.markdown('</div>', unsafe_allow_html=True)

# Page routing
if selected_model == "üè† Overview":
    st.markdown("## üåü Welcome to the Neural Architecture Framework")
    
    st.markdown("""
    ### üöÄ Revolutionary Deep Learning Framework
    
    Experience the future of deep learning with **automatic performance optimizations** that require zero configuration!
    Our framework provides seamless acceleration across multiple state-of-the-art architectures.
    """)
    
    # Show optimization status
    opt_metrics = create_optimization_metrics()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üî• Fusion Enabled", "‚úÖ Active" if opt_metrics['fusion_enabled'] else "‚ùå Disabled")
    with col2:
        st.metric("‚ö° JIT Compilation", "‚úÖ Active" if opt_metrics['jit_enabled'] else "‚ùå Disabled")
    with col3:
        st.metric("üß† Auto Backend", "‚úÖ Active" if opt_metrics['auto_backend'] else "‚ùå Disabled")
    
    st.markdown("### üèóÔ∏è Available Architectures")
    
    architectures = [
        ("üß† BERT", "Bidirectional text understanding", "Sentiment analysis, text classification"),
        ("üé≠ GPT-2", "Autoregressive text generation", "Creative writing, text completion"),
        ("üñºÔ∏è Vision Transformer", "Image classification with patches", "Computer vision, image recognition"),
        ("üß¨ Modern Transformer", "Advanced features: RoPE, SwiGLU, RMSNorm", "Next-generation language modeling"),
        ("üåü CLIP", "Multimodal vision-language understanding", "Image-text matching, zero-shot classification"),
        ("üèóÔ∏è ResNet", "Deep residual learning", "Computer vision, image classification")
    ]
    
    for name, desc, use_case in architectures:
        with st.expander(f"{name} - {desc}"):
            st.markdown(f"**Use Cases:** {use_case}")
            st.markdown("**Automatic Optimizations:** ‚úÖ Operator Fusion ‚Ä¢ ‚úÖ JIT Compilation ‚Ä¢ ‚úÖ Backend Selection")

elif selected_model == "üß† BERT":
    st.markdown("## üß† BERT - Bidirectional Text Understanding")
    
    # Model configuration
    col1, col2 = st.columns(2)
    with col1:
        model_size = st.selectbox("Model Size:", ["Small", "Base"], index=0)
        batch_size = st.slider("Batch Size:", 1, 8, 2)
    with col2:
        seq_length = st.slider("Sequence Length:", 16, 128, 32)
        vocab_size = st.slider("Vocabulary Size:", 1000, 30000, 10000)
    
    if st.button("üöÄ Run BERT Demo", type="primary"):
        with st.spinner("Initializing BERT with automatic optimizations..."):
            # Create BERT model
            if model_size == "Small":
                config = BERTConfig(
                    vocab_size=vocab_size,
                    hidden_size=256,
                    num_hidden_layers=4,
                    num_attention_heads=4,
                    intermediate_size=1024
                )
            else:
                config = BERTConfig(
                    vocab_size=vocab_size,
                    hidden_size=512,
                    num_hidden_layers=6,
                    num_attention_heads=8,
                    intermediate_size=2048
                )
            
            model = BERT(config=config)
            
            # Create sample data
            input_ids = np.random.randint(0, vocab_size, (batch_size, seq_length), dtype=np.int32)
            input_tensor = Tensor(input_ids)
            
            # Run inference
            start_time = time.time()
            outputs = model(input_tensor)
            inference_time = time.time() - start_time
            
            param_count = sum(p.data.size for p in model.parameters().values())
            
        # Display results
        display_model_card(
            f"BERT {model_size}",
            "Bidirectional Encoder Representations from Transformers",
            ["Multi-head attention", "Layer normalization", "GELU activation", "Bidirectional context"],
            param_count,
            {"Inference Time": inference_time, "Backend": input_tensor.backend.name}
        )
        
        # Show outputs
        st.markdown("### üìä Model Outputs")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Hidden States Shape", str(outputs["last_hidden_state"].shape))
            st.metric("Pooled Output Shape", str(outputs["pooler_output"].shape))
        with col2:
            st.metric("Parameters", f"{param_count:,}")
            st.metric("Inference Time", f"{inference_time:.4f}s")

elif selected_model == "üé≠ GPT-2":
    st.markdown("## üé≠ GPT-2 - Autoregressive Text Generation")
    
    # Model configuration
    col1, col2 = st.columns(2)
    with col1:
        model_size = st.selectbox("Model Size:", ["Small", "Medium"], index=0)
        batch_size = st.slider("Batch Size:", 1, 4, 2)
    with col2:
        seq_length = st.slider("Sequence Length:", 16, 64, 24)
        vocab_size = st.slider("Vocabulary Size:", 1000, 20000, 10000)
    
    if st.button("üöÄ Run GPT-2 Demo", type="primary"):
        with st.spinner("Initializing GPT-2 with automatic optimizations..."):
            # Create GPT-2 model
            if model_size == "Small":
                gpt2_config = GPT2_CONFIGS['small'].copy()
                gpt2_config.update({
                    'vocab_size': vocab_size,
                    'n_embd': 256,
                    'n_layer': 4,
                    'n_head': 4
                })
            else:
                gpt2_config = GPT2_CONFIGS['small'].copy()
                gpt2_config.update({
                    'vocab_size': vocab_size,
                    'n_embd': 512,
                    'n_layer': 6,
                    'n_head': 8
                })
            
            model = GPT2LMHead(gpt2_config)
            
            # Create sample data
            input_ids = np.random.randint(0, vocab_size, (batch_size, seq_length), dtype=np.int32)
            input_tensor = Tensor(input_ids)
            
            # Run inference
            start_time = time.time()
            outputs = model(input_tensor)
            inference_time = time.time() - start_time
            
            param_count = sum(p.data.size for p in model.parameters().values())
            
        # Display results
        display_model_card(
            f"GPT-2 {model_size}",
            "Generative Pre-trained Transformer for text generation",
            ["Causal attention", "Autoregressive generation", "RoPE positioning", "Modern architecture"],
            param_count,
            {"Inference Time": inference_time, "Backend": input_tensor.backend.name}
        )
        
        # Show outputs
        st.markdown("### üìä Model Outputs")
        if isinstance(outputs, dict):
            logits = outputs['logits']
        else:
            logits = outputs
            
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Logits Shape", str(logits.shape))
            st.metric("Vocabulary Size", f"{vocab_size:,}")
        with col2:
            st.metric("Parameters", f"{param_count:,}")
            st.metric("Inference Time", f"{inference_time:.4f}s")

elif selected_model == "üñºÔ∏è Vision Transformer":
    st.markdown("## üñºÔ∏è Vision Transformer - Image Classification")
    
    # Model configuration
    col1, col2 = st.columns(2)
    with col1:
        img_size = st.selectbox("Image Size:", [64, 128, 224], index=0)
        patch_size = st.selectbox("Patch Size:", [8, 16, 32], index=1)
    with col2:
        embed_dim = st.selectbox("Embed Dimension:", [256, 512, 768], index=1)
        num_classes = st.slider("Number of Classes:", 10, 1000, 100)
    
    if st.button("üöÄ Run ViT Demo", type="primary"):
        with st.spinner("Initializing Vision Transformer with automatic optimizations..."):
            # Create ViT model
            model = VisionTransformer(
                num_classes=num_classes,
                img_size=img_size,
                patch_size=patch_size,
                embed_dim=embed_dim,
                depth=6,
                num_heads=8
            )
            
            # Create sample data
            batch_size = 2
            images = np.random.uniform(0, 1, (batch_size, 3, img_size, img_size)).astype(np.float32)
            images_tensor = Tensor(images)
            
            # Run inference
            start_time = time.time()
            outputs = model(images_tensor)
            inference_time = time.time() - start_time
            
            param_count = sum(p.data.size for p in model.parameters().values())
            num_patches = (img_size // patch_size) ** 2
            
        # Display results
        display_model_card(
            "Vision Transformer",
            "Transformer architecture applied to image classification",
            ["Patch embeddings", "Position encoding", "Multi-head attention", "Global pooling"],
            param_count,
            {"Inference Time": inference_time, "Backend": images_tensor.backend.name}
        )
        
        # Show outputs
        st.markdown("### üìä Model Outputs")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Output Shape", str(outputs.shape))
            st.metric("Number of Patches", num_patches)
        with col2:
            st.metric("Parameters", f"{param_count:,}")
            st.metric("Inference Time", f"{inference_time:.4f}s")

elif selected_model == "üß¨ Modern Transformer":
    st.markdown("## üß¨ Modern Transformer - Advanced Architecture")
    
    # Model configuration
    col1, col2 = st.columns(2)
    with col1:
        d_model = st.selectbox("Model Dimension:", [256, 384, 512], index=1)
        num_layers = st.slider("Number of Layers:", 2, 12, 6)
    with col2:
        activation = st.selectbox("Activation:", ["gelu", "swiglu"], index=1)
        normalization = st.selectbox("Normalization:", ["layernorm", "rmsnorm"], index=1)
    
    use_rope = st.checkbox("Use RoPE (Rotary Position Embedding)", value=True)
    
    if st.button("üöÄ Run Modern Transformer Demo", type="primary"):
        with st.spinner("Initializing Modern Transformer with advanced features..."):
            # Create Modern Transformer
            config = PreNormTransformerConfig(
                d_model=d_model,
                num_layers=num_layers,
                num_heads=d_model // 64,
                d_ff=d_model * 4,
                max_seq_len=512,
                vocab_size=10000,
                activation=activation,
                normalization=normalization,
                use_rope=use_rope
            )
            model = PreNormTransformer(config)
            
            # Create sample data
            batch_size = 2
            seq_len = 32
            input_ids = np.random.randint(0, config.vocab_size, (batch_size, seq_len), dtype=np.int32)
            input_tensor = Tensor(input_ids)
            
            # Run inference
            start_time = time.time()
            outputs = model(input_tensor)
            inference_time = time.time() - start_time
            
            param_count = sum(p.data.size for p in model.parameters().values())
            
        # Display results
        features = ["Pre-Norm architecture", f"{activation.upper()} activation", f"{normalization.upper()} normalization"]
        if use_rope:
            features.append("RoPE positional encoding")
        
        display_model_card(
            "Modern Transformer",
            "Pre-Norm Transformer with cutting-edge improvements",
            features,
            param_count,
            {"Inference Time": inference_time, "Backend": input_tensor.backend.name}
        )
        
        # Show outputs
        st.markdown("### üìä Model Outputs")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Logits Shape", str(outputs['logits'].shape))
            st.metric("Model Dimension", d_model)
        with col2:
            st.metric("Parameters", f"{param_count:,}")
            st.metric("Inference Time", f"{inference_time:.4f}s")

elif selected_model == "üåü CLIP Multimodal":
    st.markdown("## üåü CLIP - Multimodal Vision-Language Understanding")
    
    # Model configuration
    col1, col2 = st.columns(2)
    with col1:
        model_size = st.selectbox("Model Size:", ["Base", "Large"], index=0)
        batch_size = st.slider("Batch Size:", 1, 4, 2)
    with col2:
        embed_dim = st.selectbox("Embedding Dimension:", [256, 512, 768], index=0)
        image_resolution = st.selectbox("Image Resolution:", [224, 288, 384], index=0)
    
    if st.button("üöÄ Run CLIP Demo", type="primary"):
        with st.spinner("Initializing CLIP with multimodal capabilities..."):
            # Create CLIP model
            if model_size.lower() == "base":
                clip_config = CLIP_CONFIGS['base'].copy()
                clip_config.update({
                    'vision_layers': 6,
                    'vision_width': 512,
                    'transformer_layers': 6,
                    'transformer_width': 384,
                    'vocab_size': 10000,
                    'embed_dim': embed_dim,
                    'image_resolution': image_resolution
                })
            else:
                clip_config = CLIP_CONFIGS['large'].copy()
                clip_config.update({
                    'vision_layers': 8,
                    'vision_width': 768,
                    'transformer_layers': 8,
                    'vocab_size': 10000,
                    'embed_dim': embed_dim,
                    'image_resolution': image_resolution
                })
            
            model = CLIP(**clip_config)
            
            # Create sample data
            images = np.random.uniform(0, 1, (batch_size, 3, image_resolution, image_resolution)).astype(np.float32)
            texts = np.random.randint(0, 10000, (batch_size, 77), dtype=np.int32)
            
            images_tensor = Tensor(images)
            texts_tensor = Tensor(texts)
            
            # Run inference
            start_time = time.time()
            outputs = model(images_tensor, texts_tensor, return_loss=True)
            inference_time = time.time() - start_time
            
            param_count = sum(p.data.size for p in model.parameters().values())
            
        # Display results
        display_model_card(
            f"CLIP {model_size}",
            "Contrastive Language-Image Pre-training for multimodal understanding",
            ["Vision Transformer encoder", "Text Transformer encoder", "Contrastive learning", "Cross-modal similarity"],
            param_count,
            {"Inference Time": inference_time, "Backend": images_tensor.backend.name}
        )
        
        # Show outputs
        st.markdown("### üìä Model Outputs")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Image Embeddings", str(outputs['image_embeds'].shape))
            st.metric("Text Embeddings", str(outputs['text_embeds'].shape))
        with col2:
            st.metric("Contrastive Loss", f"{outputs['loss'].data[0]:.4f}")
            st.metric("Inference Time", f"{inference_time:.4f}s")

elif selected_model == "üèóÔ∏è ResNet":
    st.markdown("## üèóÔ∏è ResNet - Deep Residual Learning")
    
    # Model configuration
    col1, col2 = st.columns(2)
    with col1:
        model_size = st.selectbox("ResNet Architecture:", ["ResNet-18", "ResNet-34", "ResNet-50"], index=0)
        batch_size = st.slider("Batch Size:", 1, 4, 1)
    with col2:
        image_size = st.selectbox("Image Size:", [32, 64, 128], index=0)
        num_classes = st.slider("Number of Classes:", 10, 1000, 100)
    
    if st.button("üöÄ Run ResNet Demo", type="primary"):
        with st.spinner("Initializing ResNet with residual learning..."):
            # Create ResNet model
            if model_size == "ResNet-18":
                model = ResNet18(num_classes=10, use_se=False, drop_path_rate=0.0)
                layers_config = "2-2-2-2 layers"
                block_type = "BasicBlock"
            elif model_size == "ResNet-34":
                model = ResNet34(num_classes=10, use_se=False, drop_path_rate=0.0)
                layers_config = "3-4-6-3 layers"
                block_type = "BasicBlock"
            else:  # ResNet-50
                model = ResNet50(num_classes=10, use_se=False, drop_path_rate=0.0)
                layers_config = "3-4-6-3 layers"
                block_type = "Bottleneck"
            
            # Create sample data
            images = np.random.uniform(0, 1, (batch_size, 3, image_size, image_size)).astype(np.float32)
            images_tensor = Tensor(images)
            
            # Run inference
            start_time = time.time()
            outputs = model(images_tensor)
            inference_time = time.time() - start_time
            
            param_count = sum(p.data.size for p in model.parameters().values())
            
        # Display results
        display_model_card(
            model_size,
            "Deep Residual Network with skip connections",
            ["Residual connections", "Skip connections", f"{block_type} blocks", "Batch normalization"],
            param_count,
            {"Inference Time": inference_time, "Backend": images_tensor.backend.name}
        )
        
        # Show outputs
        st.markdown("### üìä Model Outputs")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Output Shape", str(outputs.shape))
            st.metric("Layer Configuration", layers_config)
        with col2:
            st.metric("Parameters", f"{param_count:,}")
            st.metric("Inference Time", f"{inference_time:.4f}s")

elif selected_model == "üìä Model Comparison":
    st.markdown("## üìä Comprehensive Model Comparison")
    
    st.markdown("### üöÄ Run All Models Benchmark")
    
    if st.button("üî• Benchmark All Architectures", type="primary"):
        models_data = []
        
        with st.spinner("Running comprehensive benchmark across all architectures..."):
            # Progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # BERT
            status_text.text("Testing BERT...")
            config = BERTConfig(vocab_size=5000, hidden_size=256, num_hidden_layers=4, num_attention_heads=4)
            model = BERT(config=config)
            input_ids = Tensor(np.random.randint(0, 5000, (1, 32), dtype=np.int32))
            start_time = time.time()
            _ = model(input_ids)
            inference_time = time.time() - start_time
            models_data.append({
                'name': 'BERT',
                'parameters': sum(p.data.size for p in model.parameters().values()) / 1e6,
                'inference_time': inference_time,
                'architecture': 'Bidirectional Encoder'
            })
            progress_bar.progress(1/6)
            
            # GPT-2
            status_text.text("Testing GPT-2...")
            gpt2_config = GPT2_CONFIGS['small'].copy()
            gpt2_config.update({'vocab_size': 5000, 'n_embd': 256, 'n_layer': 4})
            model = GPT2LMHead(gpt2_config)
            input_ids = Tensor(np.random.randint(0, 5000, (1, 24), dtype=np.int32))
            start_time = time.time()
            _ = model(input_ids)
            inference_time = time.time() - start_time
            models_data.append({
                'name': 'GPT-2',
                'parameters': sum(p.data.size for p in model.parameters().values()) / 1e6,
                'inference_time': inference_time,
                'architecture': 'Autoregressive Decoder'
            })
            progress_bar.progress(2/6)
            
            # Vision Transformer
            status_text.text("Testing Vision Transformer...")
            model = VisionTransformer(num_classes=100, img_size=64, embed_dim=256, depth=4)
            images = Tensor(np.random.uniform(0, 1, (1, 3, 64, 64)).astype(np.float32))
            start_time = time.time()
            _ = model(images)
            inference_time = time.time() - start_time
            models_data.append({
                'name': 'ViT',
                'parameters': sum(p.data.size for p in model.parameters().values()) / 1e6,
                'inference_time': inference_time,
                'architecture': 'Transformer for Vision'
            })
            progress_bar.progress(3/6)
            
            # Modern Transformer
            status_text.text("Testing Modern Transformer...")
            config = PreNormTransformerConfig(d_model=256, num_layers=4, vocab_size=5000)
            model = PreNormTransformer(config)
            input_ids = Tensor(np.random.randint(0, 5000, (1, 32), dtype=np.int32))
            start_time = time.time()
            _ = model(input_ids)
            inference_time = time.time() - start_time
            models_data.append({
                'name': 'Modern Transformer',
                'parameters': sum(p.data.size for p in model.parameters().values()) / 1e6,
                'inference_time': inference_time,
                'architecture': 'Pre-Norm + Advanced Features'
            })
            progress_bar.progress(4/6)
            
            # CLIP
            status_text.text("Testing CLIP...")
            clip_config = {
                'embed_dim': 256, 'image_resolution': 64, 'vision_layers': 4,
                'vision_width': 256, 'transformer_layers': 4, 'transformer_width': 256, 'vocab_size': 5000
            }
            model = CLIP(**clip_config)
            images = Tensor(np.random.uniform(0, 1, (1, 3, 64, 64)).astype(np.float32))
            texts = Tensor(np.random.randint(0, 5000, (1, 77), dtype=np.int32))
            start_time = time.time()
            _ = model(images, texts)
            inference_time = time.time() - start_time
            models_data.append({
                'name': 'CLIP',
                'parameters': sum(p.data.size for p in model.parameters().values()) / 1e6,
                'inference_time': inference_time,
                'architecture': 'Multimodal Vision-Language'
            })
            progress_bar.progress(5/6)
            
            # ResNet
            status_text.text("Testing ResNet...")
            model = ResNet18(num_classes=10, use_se=False)
            images = Tensor(np.random.uniform(0, 1, (1, 3, 32, 32)).astype(np.float32))
            start_time = time.time()
            _ = model(images)
            inference_time = time.time() - start_time
            models_data.append({
                'name': 'ResNet-18',
                'parameters': sum(p.data.size for p in model.parameters().values()) / 1e6,
                'inference_time': inference_time,
                'architecture': 'Residual Network'
            })
            progress_bar.progress(6/6)
            
            status_text.text("Benchmark complete!")
        
        # Display results
        st.markdown("### üèÜ Benchmark Results")
        
        # Performance chart
        fig = create_performance_chart(models_data)
        st.plotly_chart(fig, use_container_width=True)
        
        # Results table
        st.markdown("### üìã Detailed Results")
        import pandas as pd
        df = pd.DataFrame(models_data)
        df['parameters'] = df['parameters'].apply(lambda x: f"{x:.2f}M")
        df['inference_time'] = df['inference_time'].apply(lambda x: f"{x:.4f}s")
        df.columns = ['Model', 'Parameters', 'Inference Time', 'Architecture']
        st.dataframe(df, use_container_width=True)
        
        # Summary metrics
        st.markdown("### üìä Summary Statistics")
        col1, col2, col3 = st.columns(3)
        with col1:
            total_params = sum([model['parameters'] for model in models_data])
            st.metric("Total Parameters", f"{total_params:.1f}M")
        with col2:
            avg_time = np.mean([model['inference_time'] for model in models_data])
            st.metric("Average Inference Time", f"{avg_time:.4f}s")
        with col3:
            st.metric("Architectures Tested", len(models_data))

# Footer
st.markdown("---")
st.markdown("### üéØ Framework Features")

col1, col2, col3 = st.columns(3)
with col1:
    st.markdown("""
    **üöÄ Automatic Optimizations**
    - Operator fusion
    - JIT compilation  
    - Backend selection
    - Memory optimization
    """)

with col2:
    st.markdown("""
    **üß† Advanced Architectures**
    - BERT & GPT-2
    - Vision Transformers
    - Modern Transformers
    - Multimodal CLIP & ResNet
    """)

with col3:
    st.markdown("""
    **‚ö° Zero Configuration**
    - No setup required
    - Automatic acceleration
    - Hardware adaptive
    - Production ready
    """)

st.markdown("---")
st.markdown(
    '<p style="text-align: center; color: #666;">üöÄ Neural Architecture Framework - The Future of Deep Learning</p>', 
    unsafe_allow_html=True
)